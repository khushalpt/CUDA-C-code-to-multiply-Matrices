{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1106937w3p2_cuda.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhDXKvG2U8pj"
      },
      "source": [
        "1. Check CUDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj0YGEMwU5Z8",
        "outputId": "108ba43e-d27e-4418-ab49-02fa860f1873",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!/opt/bin/nvidia-smi"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Nov 12 21:57:28 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j38fpXcYVEb9"
      },
      "source": [
        "2. Install nvcc4jupyter (nvcc_plugin) and load the plugin\n",
        "The code uses the plug in : nvcc_plugin from //github.com/andreinechaev/nvcc4jupyter.git to allow the CUDA C code to be written and executable in Colab. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwBPYNZdVGsb",
        "outputId": "1e79e60c-9c4b-4c13-cae1-2aa0ff652717",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-ewm7o2g9\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-ewm7o2g9\n",
            "Requirement already satisfied (use --upgrade to upgrade): NVCCPlugin==0.0.2 from git+git://github.com/andreinechaev/nvcc4jupyter.git in /usr/local/lib/python3.6/dist-packages\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp36-none-any.whl size=4307 sha256=94dc8349b201be78bbaea6d6f5153ddeffbf1707d27a54e6915aafb50beb0988\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-los8db74/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO9G7_UaVLYU",
        "outputId": "f546b4e2-9cc0-4e17-8be3-a7ff1bb206db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The nvcc_plugin extension is already loaded. To reload it, use:\n",
            "  %reload_ext nvcc_plugin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekANETD2VNm8"
      },
      "source": [
        "3. Write the cuda code (%%cuda --name matrix.cu )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCvzNTjFVTMD",
        "outputId": "f87f2a04-e91d-450f-91fe-5984bc745e49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%cuda --name matrix_cublas.cu \n",
        "\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "#include <cublas_v2.h>\n",
        "\n",
        "\n",
        "#define CUDA_CALL(x) do { if ((x) != cudaSuccess) { \\\n",
        "    printf(\"Error at %s:%d\\n\", __FILE__, __LINE__);\\\n",
        "    return EXIT_FAILURE; }} while(0)\n",
        "\n",
        "#define MAX_VAL 100\n",
        "\n",
        "\n",
        "cudaError_t checkCuda();\n",
        "\n",
        "__host__ __device__ void printmat(float* M, int W, int H)\n",
        "{\n",
        "    printf(\"\\n\");\n",
        "    for(int y = 0; y < H; ++y)\n",
        "    {\n",
        "      for(int x = 0; x < W; ++x)\n",
        "      {\n",
        "          printf(\"%f\\t\", M[y * W + x]);\n",
        "      }\n",
        "      printf(\"\\n\");\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "// random generation of a matrix\n",
        "\n",
        "void randomInit(float* data, int size)\n",
        "{\n",
        "    for(int i = 0; i < size; ++i)\n",
        "    {\n",
        "        data[i] = rand() / (float)RAND_MAX * MAX_VAL;\n",
        "    }\n",
        "}\n",
        "\n",
        "int matrixMultiply(dim3& dimsA, dim3& dimsB, int N)\n",
        "{\n",
        "    // Allocate host and device memory for matrices A, B and C\n",
        "    unsigned int size_A = dimsA.x * dimsA.y;\n",
        "    unsigned int mem_size_A = sizeof(float) * size_A;\n",
        "    float* h_A = NULL;\n",
        "    CUDA_CALL(cudaHostAlloc(&h_A, mem_size_A, cudaHostAllocDefault));\n",
        "    float* d_A = NULL;\n",
        "    CUDA_CALL(cudaMalloc(&d_A, mem_size_A));\n",
        "\n",
        "    unsigned int size_B = dimsB.x * dimsB.y;\n",
        "    unsigned int mem_size_B = sizeof(float) * size_B;\n",
        "    float* h_B = NULL;\n",
        "    CUDA_CALL(cudaHostAlloc(&h_B, mem_size_B, cudaHostAllocDefault));\n",
        "    float* d_B = NULL;\n",
        "    CUDA_CALL(cudaMalloc(&d_B, mem_size_B));\n",
        "\n",
        "    dim3 dimsC(dimsB.x, dimsA.y, 1);\n",
        "    unsigned int size_C = dimsC.x * dimsC.y;\n",
        "    unsigned int mem_size_C = sizeof(float) * size_C;\n",
        "    float *h_C = NULL;\n",
        "    CUDA_CALL(cudaHostAlloc(&h_C, mem_size_C, cudaHostAllocDefault));\n",
        "    float *d_C = NULL;\n",
        "    CUDA_CALL(cudaMalloc(&d_C, mem_size_C));\n",
        "\n",
        "    // random fill host matrix B\n",
        "    srand(42);\n",
        "    randomInit(h_B, size_B);\n",
        "\n",
        "    // copy host matrix B to device memory\n",
        "    CUDA_CALL(cudaMemcpy(d_B, h_B, mem_size_B, cudaMemcpyHostToDevice));\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Allocating CUDA events that will be used for timing\n",
        "    cudaEvent_t start, stop;\n",
        "    CUDA_CALL(cudaEventCreate(&start));\n",
        "    CUDA_CALL(cudaEventCreate(&stop));\n",
        "\n",
        "    // Starting\n",
        "    CUDA_CALL(cudaEventRecord(start, NULL));\n",
        "\n",
        "    const float alpha = 1.0f;\n",
        "    const float beta  = 0.0f;\n",
        "    cublasHandle_t handle;\n",
        "    CUDA_CALL(cublasCreate(&handle));\n",
        "\n",
        "    for(int i = 0; i < N; ++i)\n",
        "    {\n",
        "        // random fill host matrix A\n",
        "        randomInit(h_A, size_A);\n",
        "\n",
        "        // copy host matrix h_A to device memory d_A\n",
        "        CUDA_CALL(cudaMemcpy(d_A, h_A, mem_size_A, cudaMemcpyHostToDevice));\n",
        "\n",
        "        // formulating matrix C, C = A * B using cuBlas\n",
        "        CUDA_CALL(cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, dimsB.x, dimsA.y, dimsA.x, &alpha, d_B, dimsB.x, d_A, dimsA.x, &beta, d_C, dimsB.x));\n",
        "\n",
        "        // copying the results from device to host\n",
        "        CUDA_CALL(cudaMemcpy(h_C, d_C, mem_size_C, cudaMemcpyDeviceToHost));\n",
        "    }\n",
        "\n",
        "    // Recording the event stoppage\n",
        "    CUDA_CALL(cudaEventRecord(stop, NULL));\n",
        "    CUDA_CALL(cudaEventSynchronize(stop));\n",
        "\n",
        "    float msec_TotalMatrixMul = 0.0f;\n",
        "    CUDA_CALL(cudaEventElapsedTime(&msec_TotalMatrixMul, start, stop));\n",
        "\n",
        "    // Compute and print the performance of the algorithm\n",
        "    float msec_MatrixMul = msec_TotalMatrixMul / N;\n",
        "    double flops_MatrixMul = 2.0 * (double)dimsA.x * (double)dimsA.y * (double)dimsB.x;\n",
        "    double flopsGiga = (flops_MatrixMul * 1.0e-9f) / (msec_MatrixMul / 1000.0f);\n",
        "    printf(\"Performance= %.2f GFlop/s, AVGTime= %.3f msec, TotalTime=%.3f msc \\n\",\n",
        "        flopsGiga, msec_MatrixMul, msec_TotalMatrixMul);\n",
        " \n",
        "\n",
        "    // Memory Clean up \n",
        "    cudaFreeHost(h_A);\n",
        "    cudaFreeHost(h_B);\n",
        "    cudaFreeHost(h_C);\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "\n",
        "    return EXIT_SUCCESS;\n",
        "} \n",
        "\n",
        "int main(int argc, char** argv)\n",
        "{\n",
        "    if(checkCuda() != cudaSuccess)\n",
        "    {\n",
        "        return 0;\n",
        "    }\n",
        " \n",
        "    // condition i) A(500*500), B(500*400), N=100\n",
        "    printf(\"Performing condition i: A(500*500), B(500*400), N=100\\n\");\n",
        "    int N1 = 100;\n",
        "    dim3 dimsA1(500, 500, 1);\n",
        "    dim3 dimsB1(400, 500, 1);\n",
        "    matrixMultiply(dimsA1, dimsB1, N1);\n",
        "    printf(\"\\n\");\n",
        " \n",
        "    // condition ii) A(50*20), B(20*50), N=5000\n",
        "    printf(\"Performing condition ii: A(50*20), B(20*50), N=5000\\n\");\n",
        "    int N2 = 5000;\n",
        "    dim3 dimsA2(20, 50, 1);\n",
        "    dim3 dimsB2(50, 20, 1);\n",
        "    matrixMultiply(dimsA2, dimsB2, N2);\n",
        "    printf(\"\\n\");\n",
        " \n",
        "    // condition iii) A(6*4000), B(4000*9), N=1000\n",
        "    printf(\"Performing condition iii: A(6*4000), B(4000*9), N=1000\\n\");\n",
        "    int N3 = 1000;\n",
        "    dim3 dimsA3(4000, 6, 1);\n",
        "    dim3 dimsB3(9, 4000, 1);\n",
        "    matrixMultiply(dimsA3, dimsB3, N3);\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "cudaError_t checkCuda()\n",
        "{\n",
        "    printf(\"Checking CUDA...\\n\");\n",
        "\n",
        "    int devID = 0;\n",
        "    cudaError_t error;\n",
        "    cudaDeviceProp deviceProp;\n",
        "    error = cudaGetDevice(&devID);\n",
        "\n",
        "    if (error != cudaSuccess)\n",
        "    {\n",
        "        printf(\"cudaGetDevice returned error %s (code %d), line(%d)\\n\", cudaGetErrorString(error), error, __LINE__);\n",
        "        return error;\n",
        "    }\n",
        "\n",
        "    error = cudaGetDeviceProperties(&deviceProp, devID);\n",
        "\n",
        "    if (deviceProp.computeMode == cudaComputeModeProhibited)\n",
        "    {\n",
        "        fprintf(stderr, \"Error: device is running in <Compute Mode Prohibited>, no threads can use ::cudaSetDevice().\\n\");\n",
        "        exit(EXIT_SUCCESS);\n",
        "    }\n",
        "\n",
        "    if (error != cudaSuccess)\n",
        "    {\n",
        "        printf(\"cudaGetDeviceProperties returned error %s (code %d), line(%d)\\n\", cudaGetErrorString(error), error, __LINE__);\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        printf(\"GPU Device %d: \\\"%s\\\" with computation capability %d.%d\\n\\n\", devID, deviceProp.name, deviceProp.major, deviceProp.minor);\n",
        "    }\n",
        "\n",
        "    return error;\n",
        "}"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File written in /content/src/matrix_cublas.cu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoJ6-YviVWEE",
        "outputId": "19973869-2bd3-4fb8-b1fd-45946be92a08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvcc -o /content/src/matrix_cublas /content/src/matrix_cublas.cu -lcublas"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[01m\u001b[K/content/src/matrix_cublas.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint matrixMultiply(dim3&, dim3&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/src/matrix_cublas.cu:86:52:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between ‘\u001b[01m\u001b[Kenum cublasStatus_t\u001b[m\u001b[K’ and ‘\u001b[01m\u001b[Kenum cudaError\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wenum-compare\u001b[m\u001b[K]\n",
            "     CUDA_CALL(cublasCreate(&handle));\n",
            "                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/src/matrix_cublas.cu:97:160:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between ‘\u001b[01m\u001b[Kenum cublasStatus_t\u001b[m\u001b[K’ and ‘\u001b[01m\u001b[Kenum cudaError\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wenum-compare\u001b[m\u001b[K]\n",
            "         CUDA_CALL(cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, dimsB.x, dimsA.y, dimsA.x, &alpha, d_B, dimsB.x, d_A, dimsA.x, &beta, d_C, dimsB.x));\n",
            "                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKz6vCTKYwGY"
      },
      "source": [
        "4. Print the Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG27Ow9CVXt6",
        "outputId": "8aefa94b-b174-40ed-ed15-aa656d79173a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!/content/src/matrix_cublas"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking CUDA...\n",
            "GPU Device 0: \"Tesla T4\" with computation capability 7.5\n",
            "\n",
            "Performing condition i: A(500*500), B(500*400), N=100\n",
            "Performance= 23.47 GFlop/s, AVGTime= 8.522 msec, TotalTime=852.180 msc \n",
            "\n",
            "Performing condition ii: A(50*20), B(20*50), N=5000\n",
            "Performance= 1.81 GFlop/s, AVGTime= 0.055 msec, TotalTime=276.462 msc \n",
            "\n",
            "Performing condition iii: A(6*4000), B(4000*9), N=1000\n",
            "Performance= 0.83 GFlop/s, AVGTime= 0.518 msec, TotalTime=518.184 msc \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}